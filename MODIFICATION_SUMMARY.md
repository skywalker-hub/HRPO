# éšå¼å‘é‡è®¡ç®—æ–¹å¼ä¿®æ”¹æ€»ç»“

## ğŸ“‹ ä¿®æ”¹æ¦‚è¿°

æœ¬æ¬¡ä¿®æ”¹å°†æ¨¡å‹è®­ç»ƒæ—¶éšå¼å‘é‡(residual)çš„è®¡ç®—æ–¹å¼ä»**å›ºå®šçš„è¯è¡¨embeddingåŠ æƒæ±‚å’Œ**æ”¹ä¸º**å¯å­¦ä¹ çš„çº¿æ€§æŠ•å½±å±‚æ˜ å°„**ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç›¸å…³çš„è¿ç»­æ€ç»´è¡¨ç¤ºã€‚

---

## âœ… å®Œæˆçš„ä¿®æ”¹

### 1. **Qwen2Model æ·»åŠ  thinking_projection å±‚**
**æ–‡ä»¶**: `transformers/models/qwen2/modeling_qwen2.py`

**ä¿®æ”¹å†…å®¹**:
- åœ¨ `__init__` ä¸­æ·»åŠ äº† `thinking_projection` çº¿æ€§å±‚ (ç¬¬522è¡Œ)
- æ·»åŠ äº† `_init_thinking_projection()` åˆå§‹åŒ–æ–¹æ³• (ç¬¬530-533è¡Œ)
- ä½¿ç”¨å°æ–¹å·®é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ– (std=0.001)ï¼Œç¡®ä¿åˆå§‹è¡Œä¸ºä¸åŸæ¨¡å‹ä¸€è‡´

```python
self.thinking_projection = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
```

---

### 2. **LlamaModel æ·»åŠ  thinking_projection å±‚**
**æ–‡ä»¶**: `transformers/models/llama/modeling_llama.py`

**ä¿®æ”¹å†…å®¹**:
- åŒæ ·åœ¨ `__init__` ä¸­æ·»åŠ  `thinking_projection` (ç¬¬544è¡Œ)
- æ·»åŠ äº†ç›¸åŒçš„åˆå§‹åŒ–æ–¹æ³• (ç¬¬552-555è¡Œ)

---

### 3. **ä¿®æ”¹ç”Ÿæˆé€»è¾‘ä¸­çš„ last_thinking_states è®¡ç®—**
**æ–‡ä»¶**: `transformers/generation/utils.py`

**ä¿®æ”¹ä½ç½®**: `_sample` æ–¹æ³•ï¼Œç¬¬3370-3386è¡Œ

**åŸæ–¹æ³•**:
```python
last_thinking_states = torch.einsum(
    'bv,vd->bd', probs, self.get_input_embeddings().weight
)
last_thinking_states /= torch.sqrt((probs ** 2).sum(-1, keepdim=True)).to(last_thinking_states.dtype)
```

**æ–°æ–¹æ³•**:
```python
if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:
    # ä»æ¨¡å‹è¾“å‡ºçš„hidden stateç”Ÿæˆ
    last_token_hidden = outputs.last_hidden_state[:, -1, :]
    last_thinking_states = self.model.thinking_projection(last_token_hidden)
    # å½’ä¸€åŒ–
    last_thinking_states = last_thinking_states / (
        torch.norm(last_thinking_states, dim=-1, keepdim=True) + 1e-8
    )
else:
    # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸæ–¹æ³•
    last_thinking_states = torch.einsum(...)
```

**å…³é”®æ”¹è¿›**:
- âœ… ä»æ¦‚ç‡åˆ†å¸ƒåŠ æƒ â†’ åŸºäºå®Œæ•´hidden stateçš„å¯å­¦ä¹ æ˜ å°„
- âœ… åŒ…å«æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- âœ… ä¿ç•™äº†é™çº§æ–¹æ¡ˆç¡®ä¿å…¼å®¹æ€§

---

### 4. **ä¼˜åŒ–å™¨é…ç½®æ”¯æŒ thinking_projection**
**æ–‡ä»¶**: `patch.py`

**ä¿®æ”¹å†…å®¹**:
- å‡½æ•°ç­¾åæ·»åŠ  `lr_thinking_projection` å‚æ•° (ç¬¬5è¡Œ)
- æ·»åŠ ç‹¬ç«‹çš„ä¼˜åŒ–å™¨å‚æ•°ç»„ (ç¬¬46-52è¡Œ)
- é»˜è®¤å­¦ä¹ ç‡: `1e-4`
- æ’é™¤é€»è¾‘ä¸­æ·»åŠ  `thinking_projection` è¿‡æ»¤ (ç¬¬20ã€27è¡Œ)

```python
{
    "params": [
        p for n, p in opt_model.named_parameters() 
        if ("thinking_projection" in n and p.requires_grad)
    ],
    "lr": lr_thinking_projection,
    "weight_decay": self.args.weight_decay,
}
```

---

### 5. **è®­ç»ƒè„šæœ¬é…ç½®æ›´æ–°**
**æ–‡ä»¶**: `hrpo_gsm8k.py`

**ä¿®æ”¹å†…å®¹**:

1. **PEFT modules_to_save** (ç¬¬55è¡Œ):
```python
modules_to_save = [
    "thinking_residual_gate_r",
    "thinking_residual_gate_i",
    "thinking_residual_Lambda",
    "thinking_projection",  # æ–°å¢
]
```

2. **patch_trainer_optimizer è°ƒç”¨** (ç¬¬106è¡Œ):
```python
patch_trainer_optimizer(
    trainer,
    args.lr_residual_gate,
    args.lr_residual_Lambda,
    args.lr_thinking_projection,  # æ–°å¢
)
```

3. **å‘½ä»¤è¡Œå‚æ•°** (ç¬¬121è¡Œ):
```python
parser.add_argument("--lr_thinking_projection", type=float, default=1e-4)
```

---

### 6. **Unsloth PEFT æ”¯æŒ**
**æ–‡ä»¶**: `unsloth/models/llama.py`

**ä¿®æ”¹ä½ç½®**: ç¬¬2479-2483è¡Œ

**ä¿®æ”¹å†…å®¹**:
```python
if "thinking_projection" in module:
    assert(hasattr(model.model.model.thinking_projection, "modules_to_save"))
    model.model.model.thinking_projection.modules_to_save.default\
        .to(device = "cuda", dtype = new_dtype, non_blocking = True)
    model.model.model.thinking_projection.modules_to_save.default.requires_grad_(True)
```

ç¡®ä¿ `thinking_projection` åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­è¢«æ­£ç¡®å¤„ç†ã€‚

---

## ğŸ¯ è®¾è®¡ç‰¹ç‚¹

### 1. **å¹³æ»‘è¿‡æ¸¡åˆå§‹åŒ–**
- ä½¿ç”¨æå°æ–¹å·® (std=0.001) åˆå§‹åŒ–
- ç¡®ä¿è®­ç»ƒåˆæœŸè¡Œä¸ºæ¥è¿‘åŸæ¨¡å‹
- è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å­¦ä¹ æ›´å¥½çš„è¡¨ç¤º

### 2. **ç‹¬ç«‹å­¦ä¹ ç‡**
- `thinking_projection`: 1e-4 (é»˜è®¤)
- `thinking_residual_gate`: 1e-4
- `thinking_residual_Lambda`: 1e-3
- ä¸»æ¨¡å‹LoRA: 5e-6

### 3. **é™çº§å…¼å®¹**
- ä¿ç•™äº†åŸå§‹æ–¹æ³•ä½œä¸ºfallback
- ç¡®ä¿åœ¨æ²¡æœ‰hidden_statesæ—¶ä»èƒ½æ­£å¸¸å·¥ä½œ

### 4. **å½’ä¸€åŒ–ä¸€è‡´æ€§**
- æ–°æ–¹æ³•ä½¿ç”¨L2èŒƒæ•°å½’ä¸€åŒ–
- ä¸åŸæ–¹æ³•çš„å½’ä¸€åŒ–æ–¹å¼ä¿æŒä¸€è‡´
- ç»´æŒå‘é‡è§„æ¨¡çš„ç¨³å®šæ€§

---

## ğŸ” æ ¸å¿ƒæ”¹è¿›

### **åŸæ–¹æ³•çš„å±€é™**:
```
probs (vocabåˆ†å¸ƒ) â†’ è¯è¡¨embeddingåŠ æƒå’Œ â†’ last_thinking_states
```
- âŒ å›ºå®šæ˜ å°„ï¼Œæ— æ³•å­¦ä¹ 
- âŒ ä»…ä¾èµ–æ¦‚ç‡åˆ†å¸ƒï¼Œä¿¡æ¯æœ‰é™
- âŒ å—è¯è¡¨embeddingè´¨é‡é™åˆ¶

### **æ–°æ–¹æ³•çš„ä¼˜åŠ¿**:
```
hidden_state (å®Œæ•´ä¸Šä¸‹æ–‡) â†’ å¯å­¦ä¹ æŠ•å½± â†’ last_thinking_states
```
- âœ… å¯å­¦ä¹ çš„éçº¿æ€§æ˜ å°„
- âœ… åŸºäºå®Œæ•´hidden stateï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
- âœ… èƒ½å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„æ€ç»´è¡¨ç¤º
- âœ… ä¸å—è¯è¡¨embeddingé™åˆ¶

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

1. **è¡¨è¾¾èƒ½åŠ›æå‡**: å¯å­¦ä¹ æ˜ å°„èƒ½æ•è·æ›´å¤æ‚çš„æ€ç»´æ¨¡å¼
2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: åŸºäºå®Œæ•´hidden stateï¼ŒåŒ…å«ä½ç½®å’Œè¯­ä¹‰ä¿¡æ¯
3. **ä»»åŠ¡é€‚åº”æ€§**: è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ æœ€é€‚åˆå½“å‰ä»»åŠ¡çš„è¡¨ç¤º
4. **å¹³æ»‘æ”¶æ•›**: å°åˆå§‹åŒ–ç¡®ä¿è®­ç»ƒç¨³å®šæ€§

---

## ğŸ§ª ä½¿ç”¨æ–¹æ³•

### **è®­ç»ƒå‘½ä»¤ç¤ºä¾‹**:
```bash
python hrpo_gsm8k.py \
  --model_name Qwen/Qwen2.5-1.5B-Instruct \
  --lora_rank 32 \
  --lr 5e-6 \
  --lr_residual_gate 1e-4 \
  --lr_residual_Lambda 1e-3 \
  --lr_thinking_projection 1e-4 \
  --group_size 4 \
  --temperature 0.5
```

### **è°ƒæ•´ thinking_projection å­¦ä¹ ç‡**:
- è¾ƒå¤§å€¼ (1e-3): æ›´å¿«å­¦ä¹ ï¼Œä½†å¯èƒ½ä¸ç¨³å®š
- é»˜è®¤å€¼ (1e-4): å¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œç¨³å®šæ€§
- è¾ƒå°å€¼ (1e-5): æ›´ä¿å®ˆï¼Œæ›´æ¥è¿‘åŸå§‹è¡Œä¸º

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å‚æ•°é‡å¢åŠ **: æ¯ä¸ªæ¨¡å‹å¢åŠ  `hidden_sizeÂ²` ä¸ªå‚æ•°
   - å¯¹äºhidden_size=2048: å¢åŠ çº¦ 4M å‚æ•°
   - å¯¹äºhidden_size=4096: å¢åŠ çº¦ 16M å‚æ•°

2. **è®¡ç®—å¼€é”€**: æ¯ä¸ªç”Ÿæˆæ­¥éª¤å¢åŠ ä¸€æ¬¡çº¿æ€§å˜æ¢
   - ç›¸å¯¹äºæ•´ä½“è®¡ç®—é‡ï¼Œå¼€é”€å¾ˆå°

3. **å…¼å®¹æ€§**: 
   - éœ€è¦ç¡®ä¿ç”Ÿæˆæ—¶ `output_hidden_states=True`
   - æ—§checkpointéœ€è¦é‡æ–°è®­ç»ƒï¼ˆæ–°å¢äº†å‚æ•°ï¼‰

4. **ç›‘æ§å»ºè®®**:
   - è§‚å¯Ÿ `thinking_projection` æƒé‡çš„å˜åŒ–
   - å¯¹æ¯”æ–°æ—§æ–¹æ³•ç”Ÿæˆçš„ `last_thinking_states` å·®å¼‚
   - ç›‘æ§è®­ç»ƒåˆæœŸçš„ç¨³å®šæ€§

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶æ¸…å•

1. âœ… `transformers/models/qwen2/modeling_qwen2.py`
2. âœ… `transformers/models/llama/modeling_llama.py`
3. âœ… `transformers/generation/utils.py`
4. âœ… `patch.py`
5. âœ… `hrpo_gsm8k.py`
6. âœ… `unsloth/models/llama.py`

---

## ğŸš€ åç»­ä¼˜åŒ–å»ºè®®

1. **å¯é€‰Layer Norm**: åœ¨projectionåæ·»åŠ LayerNormç¨³å®šè®­ç»ƒ
2. **å¯é€‰Dropout**: æ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
3. **å¯é…ç½®å¼€å…³**: æ·»åŠ configé€‰é¡¹åœ¨æ–°æ—§æ–¹æ³•é—´åˆ‡æ¢
4. **ç›‘æ§å·¥å…·**: æ·»åŠ æ—¥å¿—è®°å½•thinking_projectionçš„ç»Ÿè®¡ä¿¡æ¯
5. **å¤šå±‚æŠ•å½±**: è€ƒè™‘ä½¿ç”¨2å±‚MLPæ›¿ä»£å•å±‚çº¿æ€§å˜æ¢

---

## ğŸ“š ç†è®ºä¾æ®

è¿™ä¸ªæ”¹åŠ¨åŸºäºä»¥ä¸‹è§‚å¯Ÿï¼š
1. Hidden statesåŒ…å«æ¯”softmaxæ¦‚ç‡åˆ†å¸ƒæ›´ä¸°å¯Œçš„ä¿¡æ¯
2. å¯å­¦ä¹ çš„æ˜ å°„èƒ½å¤Ÿé€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚
3. å°åˆå§‹åŒ–ç¡®ä¿è®­ç»ƒçš„å¹³æ»‘æ€§å’Œç¨³å®šæ€§
4. å½’ä¸€åŒ–ä¿æŒäº†ä¸åŸæ–¹æ³•çš„æ•°å€¼ä¸€è‡´æ€§

---

**ä¿®æ”¹å®Œæˆæ—¶é—´**: 2025-10-17
**ä¿®æ”¹è€…**: AI Assistant
**æµ‹è¯•çŠ¶æ€**: å¾…æµ‹è¯•

